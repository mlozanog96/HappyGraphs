{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = {}\n",
    "\n",
    "with open(\"C:/Users/joana/Documents/GitHub/2023SSBIPMHWR/BigData/HappyGraphs/API_Keys\", \"r\") as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            key, value = line.split(\" = \")\n",
    "            keys[key] = value.strip(\"'\")\n",
    "\n",
    "# Now you can access the values using the key names\n",
    "openai_api_key = keys[\"openai_secret\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = openai_api_key\n",
    "indicator = 'Life expectancy'\n",
    "df_year_max = 2000\n",
    "df_year_min = 2020\n",
    "countries = 'World' #, Mexico, Germany'\n",
    "trend = '▲' #▼\n",
    "\n",
    "prompt_reason_trend1 = 'summarize why has '+ indicator + ' changed over the last ' + str(df_year_max - df_year_min) + ' in ' + countries + ' so much, in under 400 tokens. Put the emphasis on the positive change in all countries.'\n",
    "prompt_reason_trend2 = 'summarize why has '+ indicator + ' changed over the last ' + str(df_year_max - df_year_min) + ' in ' + countries + ' so much, in under 400 tokens.'\n",
    "prompt_reason_trend3 = 'Explain why ' + indicator + ' has ' + trend + ' from ' + str(2000) + ' to ' + str(2020) + ' so much for ' + countries + '. Use about 400 tokens per country.'\n",
    "prompt_reason_trend4 = 'You are a scientific researcher. Explain why ' + indicator + ' has ' + trend + ' from ' + str(2000) + ' to ' + str(2020) + ' so much in ' + countries + '. Use about 400 tokens per country.'\n",
    "# prompt_reason_trend5 = 'You are a journalist. Explain why ' + indicator + ' has ' + trend + ' from ' + str(2000) + ' to ' + str(2020) + ' so much. Use about 400 tokens per country.'\n",
    "# prompt_reason_trend6 = 'You are a teacher. Explain why ' + indicator + ' has ' + trend + ' from ' + str(2000) + ' to ' + str(2020) + ' so much. Use about 400 tokens per country.'\n",
    "# prompt_reason_trend7 = 'You are a tv reporter. Explain why ' + indicator + ' has ' + trend + ' from ' + str(2000) + ' to ' + str(2020) + ' so much. Use about 400 tokens per country.'\n",
    "# tested on website because of loop # prompt_reason_trend8 = 'Explain why ' + indicator + ' has ' + trend + ' from ' + str(2000) + ' to ' + str(2020) + ' so much in ' + countries + '. Use under 400 tokens'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ai_assistant(prompt, \n",
    "                 model = 'gpt-3.5-turbo',\n",
    "                 temperature = 0.5,\n",
    "                 max_tokens = 500):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model = model,\n",
    "        messages = [{'role':'user','content': prompt}],\n",
    "        temperature = temperature,\n",
    "        max_tokens = max_tokens,\n",
    "    )\n",
    "\n",
    "    content = response['choices'][0]['message']['content']\n",
    "\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response_reason_trend1 = openai.Completion.create(engine=\"text-davinci-003\", prompt=prompt_reason_trend1, max_tokens=400)\n",
    "# answer1 = response_reason_trend1.choices[0].text.strip()\n",
    "# print('prompt1' + answer1)\n",
    "\n",
    "# response_reason_trend2 = openai.Completion.create(engine=\"text-davinci-003\", prompt=prompt_reason_trend2, max_tokens=400)\n",
    "# answer2 = response_reason_trend2.choices[0].text.strip()\n",
    "# print('prompt2' + answer2)\n",
    "\n",
    "# response_reason_trend3 = openai.Completion.create(engine=\"text-davinci-003\", prompt=prompt_reason_trend3, max_tokens=400)\n",
    "# answer3 = response_reason_trend3.choices[0].text.strip()\n",
    "# print('prompt3' + answer3)\n",
    "\n",
    "# response_reason_trend4 = openai.Completion.create(engine=\"text-davinci-003\", prompt=prompt_reason_trend4, max_tokens=400)\n",
    "# answer4 = response_reason_trend4.choices[0].text.strip()\n",
    "# print('prompt4' + answer4)\n",
    "\n",
    "# response_reason_trend5 = openai.Completion.create(engine=\"text-davinci-003\", prompt=prompt_reason_trend5, max_tokens=400)\n",
    "# answer5 = response_reason_trend5.choices[0].text.strip()\n",
    "# print('prompt5' + answer5)\n",
    "\n",
    "# response_reason_trend6 = openai.Completion.create(engine=\"text-davinci-003\", prompt=prompt_reason_trend6, max_tokens=400)\n",
    "# answer6 = response_reason_trend6.choices[0].text.strip()\n",
    "# print('prompt6' + answer6)\n",
    "\n",
    "# response_reason_trend7 = openai.Completion.create(engine=\"text-davinci-003\", prompt=prompt_reason_trend7, max_tokens=400)\n",
    "# answer7 = response_reason_trend7.choices[0].text.strip()\n",
    "# print('prompt7' + answer7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer_1 = ai_assistant(prompt_reason_trend1)\n",
    "# print('p1:' + answer_1)\n",
    "\n",
    "# answer_2 = ai_assistant(prompt_reason_trend2)\n",
    "# print('p2' + answer_2)\n",
    "\n",
    "# answer_3 = ai_assistant(prompt_reason_trend3)\n",
    "# print('prompt3' +answer_3)\n",
    "\n",
    "# answer_4 = ai_assistant(prompt_reason_trend4)\n",
    "# print('prompt4' + answer_4)\n",
    "\n",
    "# answer_5 = ai_assistant(prompt_reason_trend5)\n",
    "# st.write(answer_5)\n",
    "\n",
    "# answer_6 = ai_assistant(prompt_reason_trend6)\n",
    "# st.write(answer_6)\n",
    "\n",
    "# answer_7 = ai_assistant(prompt_reason_trend7)\n",
    "# st.write(answer_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_measure = 'Summarize which measures have been taken, and by whom, to enhance '+ indicator + ' in ' + countries + ' in under 400 tokens' \n",
    "prompt_indicator = 'What is the indicator ' + indicator + ' from the Worldbank Indicators database measuring? Name the measure unit.'\n",
    "prompt_poem = 'Write me a poem on why Graphs, that show bad world bank indicators which decrease and good world bank indicators which increase, in a positiv way make us happy and inspire us to make a positive impact on the world ourselves in under 200 tokens.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graphs, a visual tale of our world's plight,\n",
      "Revealing indicators, dark or bright.\n",
      "As bad numbers decrease, hope takes flight,\n",
      "Good ones ascending, filling hearts with delight.\n",
      "\n",
      "These graphs, a mirror of our collective fate,\n",
      "Ignite a fire, urging us not to wait.\n",
      "They whisper of challenges we must abate,\n",
      "Inspiring us to act, to change our state.\n",
      "\n",
      "In their lines and curves, we find motivation,\n",
      "To forge a path towards transformation.\n",
      "With determination, we seek salvation,\n",
      "Guided by graphs, igniting dedication.\n",
      "\n",
      "Let them remind us of our power and worth,\n",
      "To heal the wounds that plague our dear Earth.\n",
      "With every action, we sow seeds of rebirth,\n",
      "For a future where joy and hope have their berth.\n",
      "\n",
      "So let these graphs, in their simple display,\n",
      "Illuminate the choices we make each day.\n",
      "May they spur us to create a world that's okay,\n",
      "Where love, compassion, and progress hold sway.\n"
     ]
    }
   ],
   "source": [
    "answer = ai_assistant(prompt_poem)\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
